{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Cannabis tweets\n",
    "Due to the large number of cannabis tweeets available in the date range of **1st May 2018** to **31st Dec 2018**, we sample out *N* = 100,000 tweets.\n",
    "\n",
    "## Sampling technique\n",
    "In order to maintain the temporal charecteristics of the tweets, we first break group all tweets by the week in which they were tweeted. We use strified sampling on each group with each of the keywords (e.g. bong, blunt, cannabis, etc.) used to mine the tweets acting as a strata. This allows us to maintain keyword statistics when sampling.\n",
    "\n",
    "There are various ways to sample *N* items from *K* items with equal probability. We use [Reservoir Sampling](https://en.wikipedia.org/wiki/Reservoir_sampling). Since we wish to maintain the relative sizes of each starata, and temporal group in the final sample too, we sample *n* tweets from each strata such that\n",
    "```\n",
    "n = (temporal_group_size / K) * (strata_size / temporal_group_size) * N = strata_size * N / K\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TWEETS_DIR = '../Data/Tweets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANNABIS_KEYWORDS = {\n",
    "    'blunt', 'bong', 'budder', 'cannabis',\n",
    "    'cbd', 'ganja', 'hash', 'hemp',\n",
    "    'indica', 'kush', 'marijuana', 'marihuana',\n",
    "    'reefer', 'sativa', 'thc', 'weed'\n",
    "}\n",
    "\n",
    "def load_data(directory):\n",
    "    dfs = []\n",
    "    for file in glob.glob(os.path.join(directory, 'Tweets*.csv')):\n",
    "        df = pd.read_csv(file, usecols=['Id', 'CreatedAt', 'Text', 'UserId'],\n",
    "                         dtype={'Id': str, 'CreatedAt': str, 'Text': str, 'UserId': str},\n",
    "                         parse_dates=['CreatedAt'])\n",
    "        df = df[df.CreatedAt >= datetime.datetime(2018, 5, 1)]\n",
    "        df['Week'] = df.CreatedAt.apply(lambda x: x.strftime('%U'))\n",
    "        for keyword in CANNABIS_KEYWORDS:\n",
    "            df[keyword] = df.Text.apply(lambda x: keyword in x.lower())\n",
    "        dfs.append(df)\n",
    "        print(f'Processed file {file}')\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file ../Data/Tweets\\Tweets-1-10-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-10-12-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-10-9-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-11-6-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-12-11-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-13-8-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-14-5-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-15-10-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-16-7-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-17-12-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-17-9-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-18-6-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-19-11-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-2-7-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-20-8-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-21-5-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-22-10-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-23-7-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-24-12-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-24-9-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-25-6-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-26-11-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-27-8-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-28-5-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-29-10-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-3-12-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-3-9-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-30-4-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-30-7-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-31-12-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-4-6-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-5-11-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-6-8-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-7-5-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-8-10-2018.csv\n",
      "Processed file ../Data/Tweets\\Tweets-9-7-2018.csv\n",
      "Number of tweets: (19081081, 21)\n",
      "                    Id           CreatedAt  \\\n",
      "0  1046655959232204800 2018-10-01 00:00:01   \n",
      "1  1046655959555289088 2018-10-01 00:00:01   \n",
      "2  1046655960155140096 2018-10-01 00:00:01   \n",
      "3  1046655960318717952 2018-10-01 00:00:01   \n",
      "4  1046655960822026240 2018-10-01 00:00:01   \n",
      "\n",
      "                                                Text              UserId Week  \\\n",
      "0  Aktivisten aus MÃ¼nchen setzen sich ein, um end...  902610049570394112   39   \n",
      "1  [01/10 08:00] BONG BONG BONG BONG BONG BONG BO...  827969223523700736   39   \n",
      "2          BONG BONG BONG BONG BONG BONG BONG BONG ðŸŒ„          4164342694   39   \n",
      "3  Hemp-isphere, Silky Smooth, Hemp Enriched Body...           558692221   39   \n",
      "4  BONG BONG BONG BONG BONG BONG BONG BONG BONG #...          2520492834   39   \n",
      "\n",
      "     thc  ganja   hemp  indica  reefer  ...   kush   hash  budder   bong  \\\n",
      "0  False  False  False   False   False  ...  False  False   False  False   \n",
      "1  False  False  False   False   False  ...  False  False   False   True   \n",
      "2  False  False  False   False   False  ...  False  False   False   True   \n",
      "3  False  False   True   False   False  ...  False  False   False  False   \n",
      "4  False  False  False   False   False  ...  False  False   False   True   \n",
      "\n",
      "     cbd  blunt  marihuana   weed  cannabis  marijuana  \n",
      "0  False  False      False  False      True      False  \n",
      "1  False  False      False  False     False      False  \n",
      "2  False  False      False  False     False      False  \n",
      "3  False  False      False  False     False      False  \n",
      "4  False  False      False  False     False      False  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df = load_data(TWEETS_DIR)\n",
    "print(f'Number of tweets: {df.shape}')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temporal_groups(df):\n",
    "    week_groups = {}\n",
    "    for tweet in df.itertuples():\n",
    "        if tweet.Week not in week_groups:\n",
    "            week_groups[tweet.Week] = {keyword: [] for keyword in CANNABIS_KEYWORDS}\n",
    "        for keyword in CANNABIS_KEYWORDS:\n",
    "            if getattr(tweet, keyword):\n",
    "                week_groups[tweet.Week][keyword].append(tweet.Index)\n",
    "    return week_groups\n",
    "\n",
    "def reservoir_sample(ls, n):\n",
    "    \"\"\"\n",
    "    Select n samples from ls with probability n / len(ls).\n",
    "    \"\"\"\n",
    "    rand = random.Random()\n",
    "    reservoir = ls[:n]\n",
    "    for i in range(n, len(ls)):\n",
    "        p = rand.randint(0, i)\n",
    "        if p < n:\n",
    "            reservoir[p] = ls[i]\n",
    "    return reservoir\n",
    "\n",
    "def temporal_stratified_sample(week_groups, n):\n",
    "    \"\"\"\n",
    "    Chooses n total samples from df while preserving temporal, and keyword statistics (per week).\n",
    "    \"\"\"\n",
    "    N = df.shape[0]\n",
    "    sample = {}\n",
    "    for week in week_groups:\n",
    "        sample[week] = {keyword: [] for keyword in CANNABIS_KEYWORDS}\n",
    "        for keyword, group in week_groups[week].items():\n",
    "            sample_size = int(len(group) / N * n)\n",
    "            sample[week][keyword] = reservoir_sample(group, sample_size)\n",
    "        print(f'Sampled for week {week}.')\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_groups = get_temporal_groups(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled for week 39.\n",
      "Sampled for week 40.\n",
      "Sampled for week 49.\n",
      "Sampled for week 50.\n",
      "Sampled for week 36.\n",
      "Sampled for week 37.\n",
      "Sampled for week 23.\n",
      "Sampled for week 24.\n",
      "Sampled for week 45.\n",
      "Sampled for week 46.\n",
      "Sampled for week 32.\n",
      "Sampled for week 33.\n",
      "Sampled for week 19.\n",
      "Sampled for week 20.\n",
      "Sampled for week 41.\n",
      "Sampled for week 42.\n",
      "Sampled for week 28.\n",
      "Sampled for week 29.\n",
      "Sampled for week 51.\n",
      "Sampled for week 38.\n",
      "Sampled for week 25.\n",
      "Sampled for week 47.\n",
      "Sampled for week 26.\n",
      "Sampled for week 27.\n",
      "Sampled for week 34.\n",
      "Sampled for week 21.\n",
      "Sampled for week 43.\n",
      "Sampled for week 30.\n",
      "Sampled for week 52.\n",
      "Sampled for week 48.\n",
      "Sampled for week 35.\n",
      "Sampled for week 22.\n",
      "Sampled for week 44.\n",
      "Sampled for week 17.\n",
      "Sampled for week 18.\n",
      "Sampled for week 31.\n"
     ]
    }
   ],
   "source": [
    "sample = temporal_stratified_sample(week_groups, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(df, sample, save_file):\n",
    "    sample_ind = list({x for week in sample for group in sample[week].values() for x in group})\n",
    "    sampled_tweets = df.iloc[sample_ind][['Id', 'UserId', 'CreatedAt', 'Text']]\n",
    "    print(f'Saving {sampled_tweets.shape[0]} tweets to {save_file}')\n",
    "    with open(save_file, 'wb') as file_handle:\n",
    "        pickle.dump(sampled_tweets, file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 102701 tweets to ../Data/Tweets/sampled.pickle\n"
     ]
    }
   ],
   "source": [
    "save_samples(df, sample, os.path.join(TWEETS_DIR, 'sampled.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TWEETS_DIR, 'sampled.pickle'), 'rb') as file_handle:\n",
    "    sdf = pickle.load(file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75751,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.UserId.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TWEETS_DIR, '../Users/botscores.pickle'), 'rb') as file_handle:\n",
    "    bs = pickle.load(file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
